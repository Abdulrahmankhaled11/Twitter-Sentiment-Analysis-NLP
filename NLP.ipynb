{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd816689-8828-4751-891a-ed51b4299974",
   "metadata": {},
   "source": [
    "## Import Librarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e121e6e-45fa-4d6e-92bd-7ff6cf086548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import re \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074175e7-71aa-45ad-9642-fcb315fa9674",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9d56e39-6212-4bd0-b678-0a53f1a272e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Sentiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5fce7-699b-4ec4-9a24-2595bd5172da",
   "metadata": {},
   "source": [
    "## Information About Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7665d3e2-3798-4372-8606-4988c6d27087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_confidence</th>\n",
       "      <th>relevant_yn</th>\n",
       "      <th>relevant_yn_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>subject_matter</th>\n",
       "      <th>subject_matter_confidence</th>\n",
       "      <th>candidate_gold</th>\n",
       "      <th>...</th>\n",
       "      <th>relevant_yn_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>subject_matter_gold</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697200650592256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199560069120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199312482304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697197118861312</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697196967903232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               candidate  candidate_confidence relevant_yn  \\\n",
       "0   1  No candidate mentioned                   1.0         yes   \n",
       "1   2            Scott Walker                   1.0         yes   \n",
       "2   3  No candidate mentioned                   1.0         yes   \n",
       "3   4  No candidate mentioned                   1.0         yes   \n",
       "4   5            Donald Trump                   1.0         yes   \n",
       "\n",
       "   relevant_yn_confidence sentiment  sentiment_confidence     subject_matter  \\\n",
       "0                     1.0   Neutral                0.6578  None of the above   \n",
       "1                     1.0  Positive                0.6333  None of the above   \n",
       "2                     1.0   Neutral                0.6629  None of the above   \n",
       "3                     1.0  Positive                1.0000  None of the above   \n",
       "4                     1.0  Positive                0.7045  None of the above   \n",
       "\n",
       "   subject_matter_confidence candidate_gold  ... relevant_yn_gold  \\\n",
       "0                     1.0000            NaN  ...              NaN   \n",
       "1                     1.0000            NaN  ...              NaN   \n",
       "2                     0.6629            NaN  ...              NaN   \n",
       "3                     0.7039            NaN  ...              NaN   \n",
       "4                     1.0000            NaN  ...              NaN   \n",
       "\n",
       "  retweet_count  sentiment_gold subject_matter_gold  \\\n",
       "0             5             NaN                 NaN   \n",
       "1            26             NaN                 NaN   \n",
       "2            27             NaN                 NaN   \n",
       "3           138             NaN                 NaN   \n",
       "4           156             NaN                 NaN   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...         NaN   \n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...         NaN   \n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...         NaN   \n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...         NaN   \n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...         NaN   \n",
       "\n",
       "               tweet_created            tweet_id  tweet_location  \\\n",
       "0  2015-08-07 09:54:46 -0700  629697200650592256             NaN   \n",
       "1  2015-08-07 09:54:46 -0700  629697199560069120             NaN   \n",
       "2  2015-08-07 09:54:46 -0700  629697199312482304             NaN   \n",
       "3  2015-08-07 09:54:45 -0700  629697197118861312           Texas   \n",
       "4  2015-08-07 09:54:45 -0700  629697196967903232             NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                       Quito  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3  Central Time (US & Canada)  \n",
       "4                     Arizona  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569325b-a33d-44ac-9824-c8933981ba0e",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Text Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11d9d1e-da6e-45b8-890c-5593e2d92f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tweet_header(text):\n",
    "    header = re.compile(r\"RT @.*:\")\n",
    "    return header.sub(r\"\" , text)\n",
    "\n",
    "#-----------------------\n",
    "def remove_mentions(text):\n",
    "    mention = re.compile(r\"@[A-Za-z0-9_]+\")\n",
    "    return mention.sub(r\"\" , text)    \n",
    "\n",
    "#-----------------------\n",
    "def remove_hashtags(text):\n",
    "    hashtag = re.compile(r\"#\\w+\")\n",
    "    return hashtag.sub(r\"\" , text) \n",
    "\n",
    "#-----------------------\n",
    "def remove_numeric(text):\n",
    "    nums = re.compile(r\"\\d\")\n",
    "    return nums.sub(r\"\",text)\n",
    "#-----------------------\n",
    "def remove_URL(text):\n",
    "    url =re.compile( r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "#-----------------------\n",
    "\n",
    "def remove_html(text):\n",
    "    html =re.compile( r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)\n",
    "\n",
    "#-----------------------\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji = re.compile(\n",
    "   \"[\"\n",
    "       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "       u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "       u\"\\U00002702-\\U000027B0\"\n",
    "       u\"\\U00002702-\\U000027B0\"\n",
    "       u\"\\U000024C2-\\U0001F251\"\n",
    "       u\"\\U0001f926-\\U0001f937\"\n",
    "       u\"\\U00010000-\\U0010ffff\"\n",
    "       u\"\\u2640-\\u2642\" \n",
    "       u\"\\u2600-\\u2B55\"\n",
    "       u\"\\u200d\"\n",
    "       u\"\\u23cf\"\n",
    "       u\"\\u23e9\"\n",
    "       u\"\\u231a\"\n",
    "       u\"\\ufe0f\"  # dingbats\n",
    "       u\"\\u3030\"\n",
    "       \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji.sub(r\"\",text)\n",
    "\n",
    "#-----------------------\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans(\"\",\"\",string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "#-----------------------\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = stopwords.words(\"english\")\n",
    "    text = [word.lower() for word in  text.split(\" \") if word not in stop]\n",
    "    return  \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5cc950-35a3-4f6c-b8c2-95a268a6c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"text\"].map(remove_tweet_header)\n",
    "x = x.map(remove_numeric)\n",
    "x = x.map(remove_mentions)\n",
    "#x= x.map(remove_hashtags)\n",
    "x = x.map(remove_URL)\n",
    "x = x.map(remove_html)\n",
    "x = x.map(remove_emojis)\n",
    "x= x.map(remove_punct)\n",
    "x = x.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd54cbe-f0a7-4d2a-b340-a27bd55843c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = []\n",
    "    for word in text.split():\n",
    "        stemmed.append(stemmer.stem(word))\n",
    "    return \" \".join(stemmed)\n",
    "\n",
    "x = x.map(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54797da9-3374-4ab9-959b-627a06363806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_generate(text , n_gram = (1,1)):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=n_gram)\n",
    "    return tfidf_vectorizer.fit_transform(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59fc687-ccdd-414d-8485-0bd3270c49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfidf = tfidf_generate(x)\n",
    "y = pd.get_dummies(df[\"sentiment\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85489215-ee81-4ad7-94c3-fbdf3f3d7e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8493\n",
       "1     3142\n",
       "22    2236\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lables = list(df[\"sentiment\"].value_counts().keys())\n",
    "y_ord = df[\"sentiment\"].replace(lables[0],0)\n",
    "y_ord = y_ord.replace(lables[1],1)\n",
    "y_ord = y_ord.replace(lables[2],2)\n",
    "y_ord.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912e066-daef-41e8-8e7a-5d8d08ea192f",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b47390d0-a727-46ec-b3de-3e544c262444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x_tfidf,y_ord,random_state = 123 , stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c8f9b-a2f3-433a-acce-f278d55c3ef0",
   "metadata": {},
   "source": [
    "## Build Model [ML]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df8b94df-2c8d-4a68-9847-c9ce938b0333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0df565-f959-4469-a502-43f06fbcc745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.65      0.73      2123\n",
      "           1       0.43      0.58      0.49       786\n",
      "          22       0.48      0.62      0.54       559\n",
      "\n",
      "    accuracy                           0.63      3468\n",
      "   macro avg       0.58      0.62      0.59      3468\n",
      "weighted avg       0.68      0.63      0.65      3468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104bc55-248c-4bdd-b715-e518c0df7046",
   "metadata": {},
   "source": [
    "---------------\n",
    "## Prepare For Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "749183a1-0ca0-4fa2-a524-38dcab0f74b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10366"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#count unique words \n",
    "def count_words(text):\n",
    "    count = Counter()\n",
    "    for sent in text.values :\n",
    "        for word in sent.split():\n",
    "            count[word]+=1\n",
    "    return count\n",
    "\n",
    "counter = count_words(x)\n",
    "num_words = len(counter)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3f5b73-95f9-476d-b0a7-7fcfe6fb1827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_len(text):\n",
    "    length = []\n",
    "    for sent in text.values :\n",
    "        count = Counter()\n",
    "        for word in sent.split():\n",
    "            count[word]+=1\n",
    "        length.append(len(count))\n",
    "    return max(length)\n",
    "\n",
    "max_length = max_len(x)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb00b695-84ac-4edd-9ebb-60ed79772458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "tokenizer.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c49bbbfa-8d50-473e-8fe8-415e52c22d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be31d685-5501-4fcf-86a0-96a26314585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_toknized_sents = tokenizer.texts_to_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e4d8ba-a9cf-4d8c-a891-09414f5a043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no mention tamir rice gopdeb held cleveland wow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[93, 213, 3364, 3365, 1, 1390, 727, 442]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x[2])\n",
    "x_toknized_sents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dca97bc7-0dbe-495d-886c-2f7cb60974ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  93,  213, 3364, 3365,    1, 1390,  727,  442,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "x_padded = pad_sequences(x_toknized_sents , maxlen= max_length,padding=\"post\")\n",
    "x_padded[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bdbe746-aeb5-42e9-b544-d3e756185d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(x_padded,y,random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2bf37-364d-4685-aa04-4fd3f56436ca",
   "metadata": {},
   "source": [
    "## Build Neural Network [LSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40bcb136-3ab6-4e09-877f-b7ee8c7f1f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "326/326 [==============================] - 31s 83ms/step - loss: 0.8768 - accuracy: 0.6159\n",
      "Epoch 2/15\n",
      "326/326 [==============================] - 27s 84ms/step - loss: 0.7264 - accuracy: 0.6807\n",
      "Epoch 3/15\n",
      "326/326 [==============================] - 28s 86ms/step - loss: 0.6235 - accuracy: 0.7515\n",
      "Epoch 4/15\n",
      "326/326 [==============================] - 28s 86ms/step - loss: 0.5228 - accuracy: 0.8065\n",
      "Epoch 5/15\n",
      "326/326 [==============================] - 28s 85ms/step - loss: 0.4487 - accuracy: 0.8369\n",
      "Epoch 6/15\n",
      "326/326 [==============================] - 28s 87ms/step - loss: 0.3953 - accuracy: 0.8594\n",
      "Epoch 7/15\n",
      "326/326 [==============================] - 29s 88ms/step - loss: 0.3485 - accuracy: 0.8785\n",
      "Epoch 8/15\n",
      "326/326 [==============================] - 29s 89ms/step - loss: 0.3274 - accuracy: 0.8831\n",
      "Epoch 9/15\n",
      "326/326 [==============================] - 28s 87ms/step - loss: 0.3116 - accuracy: 0.8903\n",
      "Epoch 10/15\n",
      "326/326 [==============================] - 27s 84ms/step - loss: 0.2965 - accuracy: 0.8930\n",
      "Epoch 11/15\n",
      "326/326 [==============================] - 29s 89ms/step - loss: 0.2783 - accuracy: 0.8985\n",
      "Epoch 12/15\n",
      "326/326 [==============================] - 28s 87ms/step - loss: 0.2768 - accuracy: 0.8991\n",
      "Epoch 13/15\n",
      "326/326 [==============================] - 29s 89ms/step - loss: 0.2677 - accuracy: 0.9002\n",
      "Epoch 14/15\n",
      "326/326 [==============================] - 28s 86ms/step - loss: 0.2484 - accuracy: 0.9058\n",
      "Epoch 15/15\n",
      "326/326 [==============================] - 29s 89ms/step - loss: 0.2492 - accuracy: 0.9023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b517c1aca0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(num_words , 128, input_length= max_length),\n",
    "        keras.layers.LSTM(128,dropout=0.1,recurrent_dropout=0.2),\n",
    "        keras.layers.Dense(128,activation = \"tanh\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(64,activation = \"tanh\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(64,activation = \"tanh\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(32,activation = \"tanh\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(32,activation = \"tanh\"),\n",
    "        keras.layers.Dense(3,activation = \"softmax\")\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "model.fit(x_train,y_train,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d35c1e0-bbb6-4b3b-9c27-20837e6f6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 2s 10ms/step - loss: 1.3214 - accuracy: 0.6286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3213591575622559, 0.6286044120788574]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ddb9a0-73ec-453b-bd57-0c4b0c041682",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f0688-5e89-4732-a83c-caafc6a8c270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bdda9-ee75-4332-b6f5-d2bc4d04d32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
